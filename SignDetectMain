import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import math
import tkinter as tk
import time
from collections import Counter

# Initialize the camera
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("Error: Could not open video stream.")
    exit()

# Initialize hand detector and classifier
detector = HandDetector(maxHands=1)
classifier = Classifier("D:/Visual Studio/Programs/SignLanguageDetection/NewModels/converted_keras (1)/keras_model.h5", 
                        "D:/Visual Studio/Programs/SignLanguageDetection/NewModels/converted_keras (1)/labels.txt")

# Constants
offset = 20
imgSize = 300

# Labels for sign language gestures
labels = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z", "Blank", "Delete"]

# Initialize Tkinter window
root = tk.Tk()
root.title("Sign Language Prediction")
root.geometry("800x300")

# Create a Label to display predictions
prediction_label = tk.Label(root, text="", font=("Helvetica", 16))
prediction_label.pack(pady=20)

# Function to update the label text
def update_label(new_text):
    current_text = prediction_label.cget("text")
    if new_text == "Blank":
        new_text = " "  # Replace "Blank" with a space
    elif new_text == "Delete":
        prediction_label.config(text=current_text[:-1])  # Delete the last character
    else:
        prediction_label.config(text=current_text + new_text)

# Variables to track predictions
prediction_counts = {label: 0 for label in labels}
current_string = ""

# Thresholds
letter_threshold = 10  # Lowered for quicker predictions
difference_threshold = 5
blank_threshold = 10

# Function to process video frames
def process_frame():
    global prediction_counts, current_string
    
    success, img = cap.read()
    if not success:
        print("Failed to grab frame")
        root.after(30, process_frame)  # Adjusted for 30ms delay (~33 fps)
        return

    # Find hands in the frame
    hands, img = detector.findHands(img)

    if hands:
        hand = hands[0]
        x, y, w, h = hand['bbox']

        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255

        # Crop the region around the hand
        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]
        imgCropShape = imgCrop.shape

        # Aspect ratio adjustment
        aspectRatio = h / w

        # Resize and place the cropped region onto a white canvas
        if aspectRatio > 1:
            k = imgSize / h
            wCal = math.ceil(k * w)
            imgResize = cv2.resize(imgCrop, (wCal, imgSize))
            wGap = math.ceil((imgSize - wCal) / 2)
            imgWhite[:, wGap: wCal + wGap] = imgResize
            # Adjust landmarks
            lmList = [(int((lm[0] - x + offset) * k + wGap), int((lm[1] - y + offset) * k)) for lm in hand['lmList']]
        else:
            k = imgSize / w
            hCal = math.ceil(k * h)
            imgResize = cv2.resize(imgCrop, (imgSize, hCal))
            hGap = math.ceil((imgSize - hCal) / 2)
            imgWhite[hGap: hCal + hGap, :] = imgResize
            # Adjust landmarks
            lmList = [(int((lm[0] - x + offset) * k), int((lm[1] - y + offset) * k + hGap)) for lm in hand['lmList']]

        # Resize black image to bbox size
        black_img = np.zeros((imgCropShape[0], imgCropShape[1], 3), np.uint8)

        # Adjust landmarks relative to the bbox
        lmList = [(lm[0] - x + offset, lm[1] - y + offset) for lm in hand['lmList']]

        # Define connections between landmarks for drawing the hand
        hand_connections = [
            (0, 1), (1, 2), (2, 3), (3, 4),  # Thumb
            (0, 5), (5, 6), (6, 7), (7, 8),  # Index finger
            (0, 9), (9, 10), (10, 11), (11, 12),  # Middle finger
            (0, 13), (13, 14), (14, 15), (15, 16),  # Ring finger
            (0, 17), (17, 18), (18, 19), (19, 20),  # Little finger
            (5, 9), (9, 13), (13, 17)  # Palm
        ]

        # Draw lines and dots on the black image using predefined connections
        for connection in hand_connections:
            start_point = lmList[connection[0]]
            end_point = lmList[connection[1]]
            cv2.line(black_img, start_point, end_point, (255, 255, 255), 2)
        for lm in lmList:
            cv2.circle(black_img, lm, 5, (0, 0, 255), cv2.FILLED)

        # Resize the black image to fit into the 300x300 imgWhite
        if aspectRatio > 1:
            black_img_resize = cv2.resize(black_img, (wCal, imgSize))
            imgWhite[:, wGap: wCal + wGap] = black_img_resize
        else:
            black_img_resize = cv2.resize(black_img, (imgSize, hCal))
            imgWhite[hGap: hCal + hGap, :] = black_img_resize

        # Get the prediction and index from the classifier
        prediction, index = classifier.getPrediction(imgWhite, draw=False)
        predicted_label = labels[index]

        # Update prediction counts
        prediction_counts[predicted_label] += 1

        # Determine if the letter or blank space should be added to the string
        if (prediction_counts[predicted_label] > letter_threshold and 
            all(prediction_counts[label] < prediction_counts[predicted_label] - difference_threshold 
                for label in labels if label != predicted_label)):
            
            if predicted_label == "Blank":
                if current_string:
                    update_label(" ")
                    current_string = ""
            elif predicted_label == "Delete":
                update_label("Delete")
            else:
                update_label(predicted_label)
                current_string = predicted_label

            # Reset prediction counts
            prediction_counts = {label: 0 for label in labels}

    else:
        # Reset prediction counts if no hands are detected
        prediction_counts = {label: 0 for label in labels}

    # Display the original image with bounding box and text
    cv2.imshow('Image', img)

    # Schedule the next call to this function
    root.after(30, process_frame)  # Adjusted for 30ms delay (~33 fps)

# Start processing frames
process_frame()

# Run the Tkinter event loop
root.mainloop()

# Release the camera and close all windows when Tkinter window is closed
cap.release()
cv2.destroyAllWindows()
